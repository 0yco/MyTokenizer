{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efed4bf2",
   "metadata": {},
   "source": [
    "# My Tokenizer\n",
    "\n",
    "This is MyTokenizer, let's build üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcabd3b4",
   "metadata": {},
   "source": [
    "#### Import and Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0a061f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import os\n",
    "\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4O_SPLIT_PATTERN = \"|\".join(\n",
    "    [\n",
    "        r\"\"\"[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?\"\"\",\n",
    "        r\"\"\"[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?\"\"\",\n",
    "        r\"\"\"\\p{N}{1,3}\"\"\",\n",
    "        r\"\"\" ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*\"\"\",\n",
    "        r\"\"\"\\s*[\\r\\n]+\"\"\",\n",
    "        r\"\"\"\\s+(?!\\S)\"\"\",\n",
    "        r\"\"\"\\s+\"\"\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3242e",
   "metadata": {},
   "source": [
    "#### MyTokenizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fdd63ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, vocab_size=276, pattern=GPT2_SPLIT_PATTERN):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.compiled_pattern = re.compile(pattern)\n",
    "        self.num_merges = vocab_size - 256\n",
    "        self.vocab = {index: bytes([index]) for index in range(256)}\n",
    "        self.merges = {}\n",
    "        self._build_vocab()\n",
    "\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        for i in range(256):\n",
    "            self.vocab[i] = bytes([i])\n",
    "        for i in range(256, self.vocab_size):\n",
    "            self.vocab[i] = bytes([i % 256]) + bytes([i // 256])\n",
    "\n",
    "    def _get_stats(self, ids, counts=None):\n",
    "        counts = {} if counts is None else counts\n",
    "        for pair in zip(ids, ids[1:]):\n",
    "            counts[pair] = counts.get(pair, 0) + 1\n",
    "        return counts\n",
    "\n",
    "    def _merge(self, ids, pair, index):\n",
    "        new_ids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "                new_ids.append(index)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_ids.append(ids[i])\n",
    "                i += 1\n",
    "        return new_ids\n",
    "\n",
    "    def train(self, text):\n",
    "        # Split the text in chunks\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "\n",
    "        # Input text processing\n",
    "        ids = [list(text_chunk.encode('utf-8')) for text_chunk in text_chunks]\n",
    "\n",
    "        print(\"üõ†Ô∏è Training tokenizer...\")\n",
    "        for i in range(self.num_merges):\n",
    "            stats = {}\n",
    "\n",
    "            for chunk_ids in ids:\n",
    "                self._get_stats(chunk_ids, stats)\n",
    "\n",
    "            # Find the pair with the highest occ\n",
    "            pair = max(stats, key=stats.get)\n",
    "            index = 256 + i\n",
    "\n",
    "            # Print every 1000 Merge\n",
    "            if index % 1000 == 0:\n",
    "                print(f\"‚öôÔ∏è Merge {i}: {pair} -> {index}\")\n",
    "\n",
    "            # Replace\n",
    "            ids = [self._merge(chunk_ids, pair, index) for chunk_ids in ids]\n",
    "            self.merges[pair] = index\n",
    "            self.vocab[index] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "\n",
    "    def load(self, file_prefix):\n",
    "        \"\"\"\n",
    "        Recharge le tokenizer √† partir des fichiers file_prefix.model et file_prefix.vocab.\n",
    "        \"\"\"\n",
    "        print(\"üîÑ Loading tokenizer...\")\n",
    "        model_file = file_prefix + \".model\"\n",
    "        if not os.path.exists(model_file):\n",
    "            print(f\"‚ö†Ô∏è Model file '{model_file}' not found. It will be created after training.\")\n",
    "            return\n",
    "        with open(model_file, 'r', encoding='utf-8') as f:\n",
    "            # Lire la version (ignor√©e ici)\n",
    "            version = f.readline().strip()\n",
    "            print(f\"üìú Version: {version}\")\n",
    "            # Lire les merges\n",
    "            self.merges = {}\n",
    "            for line in f:\n",
    "                token1, token2, idx = line.strip().split()\n",
    "                self.merges[(int(token1), int(token2))] = int(idx)\n",
    "\n",
    "        # Reconstruire le vocabulaire √† partir des merges\n",
    "        self.vocab = {i: bytes([i]) for i in range(256)}\n",
    "        for (token1, token2), idx in self.merges.items():\n",
    "            self.vocab[idx] = self.vocab[token1] + self.vocab[token2]\n",
    "\n",
    "    def save(self, file_prefix, version_name=\"my_tokenizer_v1\"):\n",
    "        \"\"\"\n",
    "        Sauvegarde deux fichiers : file_prefix.model et file_prefix.vocab\n",
    "        - Le fichier .model contient les informations n√©cessaires pour recharger le tokenizer.\n",
    "        - Le fichier .vocab est une version lisible pour inspection humaine.\n",
    "        \"\"\"\n",
    "        # Sauvegarder le fichier mod√®le\n",
    "        model_file = file_prefix + \".model\"\n",
    "        with open(model_file, 'w', encoding='utf-8') as f:\n",
    "            # √âcrire une version pour identifier le format\n",
    "            f.write(version_name + \"\\n\")\n",
    "            # √âcrire les merges\n",
    "            for pair, idx in self.merges.items():\n",
    "                f.write(f\"{pair[0]} {pair[1]} {idx}\\n\")\n",
    "\n",
    "        # Sauvegarder le fichier vocabulaire\n",
    "        vocab_file = file_prefix + \".vocab\"\n",
    "        with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "            for idx, token in self.vocab.items():\n",
    "                # Convertir les tokens en cha√Ænes de caract√®res lisibles\n",
    "                token_str = token.decode('utf-8', errors='replace')\n",
    "                # √âcrire uniquement les tokens standards (pas de tokens sp√©ciaux)\n",
    "                if idx >= 256:  # Supposons que les tokens sp√©ciaux sont en dessous de 256\n",
    "                    f.write(f\"{idx}: {token_str}\\n\")\n",
    "        print(f\"üíæ {version_name} saved to {model_file} and {vocab_file}\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Split the text in chunks\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "\n",
    "        # Identifiers\n",
    "        ids = []\n",
    "\n",
    "        for text_chunk in text_chunks:\n",
    "            text_bytes_chunk = text_chunk.encode('utf-8')\n",
    "            ids_chunk = list(text_bytes_chunk)\n",
    "            while len(ids_chunk) > 2:\n",
    "                stats = self._get_stats(ids_chunk)\n",
    "                min_pair = min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
    "                if min_pair not in self.merges:\n",
    "                    break\n",
    "                index = self.merges[min_pair]\n",
    "                ids_chunk = self._merge(ids_chunk, min_pair, index)\n",
    "            ids.extend(ids_chunk)\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = b\"\".join([self.vocab[index] for index in ids])\n",
    "        text = tokens.decode('utf-8', errors='replace')\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc2f06",
   "metadata": {},
   "source": [
    "#### My Tokenizer v1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5900b5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading tokenizer...\n",
      "‚ö†Ô∏è Model file 'my_basic_tokenizer.model' not found. It will be created after training.\n",
      "üõ†Ô∏è Training tokenizer...\n",
      "‚öôÔ∏è Merge 744: (104, 726) -> 1000\n",
      "‚öôÔ∏è Merge 1744: (323, 1871) -> 2000\n",
      "‚öôÔ∏è Merge 2744: (949, 583) -> 3000\n",
      "‚öôÔ∏è Merge 3744: (3999, 1033) -> 4000\n",
      "üíæ my_tokenizer_v1 saved to my_basic_tokenizer.model and my_basic_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "my_basic_tokenizer = MyTokenizer(vocab_size=5000)\n",
    "# Load the tokenizer from files\n",
    "my_basic_tokenizer.load('my_basic_tokenizer')\n",
    "\n",
    "# Train the tokenizer on a text file\n",
    "with open('data_code.txt', 'r', encoding='utf-8') as f:\n",
    "    my_basic_tokenizer.train(f.read())\n",
    "\n",
    "# Save the tokenizer vocab and encoder files\n",
    "my_basic_tokenizer.save('my_basic_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc39be28",
   "metadata": {},
   "source": [
    "#### My Tokenizer v2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "56e06449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading tokenizer...\n",
      "‚ö†Ô∏è Model file 'my_tokenizer.model' not found. It will be created after training.\n",
      "üõ†Ô∏è Training tokenizer...\n",
      "‚öôÔ∏è Merge 744: (32, 76) -> 1000\n",
      "‚öôÔ∏è Merge 1744: (1999, 121) -> 2000\n",
      "‚öôÔ∏è Merge 2744: (363, 2500) -> 3000\n",
      "‚öôÔ∏è Merge 3744: (3999, 305) -> 4000\n",
      "üíæ my_tokenizer_v2 saved to my_tokenizer.model and my_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "my_tokenizer = MyTokenizer(vocab_size=5000, pattern=GPT4_SPLIT_PATTERN)\n",
    "# Load the tokenizer from files\n",
    "my_tokenizer.load('my_tokenizer')\n",
    "\n",
    "# Train the tokenizer on a text file\n",
    "with open('data_code.txt', 'r', encoding='utf-8') as f:\n",
    "    my_tokenizer.train(f.read())\n",
    "\n",
    "# Save the tokenizer vocab and encoder files\n",
    "my_tokenizer.save('my_tokenizer', version_name='my_tokenizer_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b224b6",
   "metadata": {},
   "source": [
    "### Let's compare 3 differents encoding\n",
    "\n",
    "‚úñÔ∏è Vocabularies\n",
    "\n",
    "‚úîÔ∏è Vocabularies (GPT2 Pattern)\n",
    "\n",
    "‚òëÔ∏è Vocabularies Ameliored (GPT4 Pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9f181",
   "metadata": {},
   "source": [
    "#### Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7bc84f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading tokenizer...\n",
      "üìú Version: my_tokenizer_v1\n",
      "üîÑ Loading tokenizer...\n",
      "üìú Version: my_tokenizer_v2\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer_without_files = MyTokenizer()\n",
    "my_basic_tokenizer = MyTokenizer(vocab_size=5000)\n",
    "my_basic_tokenizer.load('my_basic_tokenizer')\n",
    "my_tokenizer = MyTokenizer(vocab_size=5000, pattern=GPT4_SPLIT_PATTERN)\n",
    "my_tokenizer.load('my_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0a72f",
   "metadata": {},
   "source": [
    "#### Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c8b8cd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 55\n",
      "‚úñÔ∏è Encoded: [72, 101, 108, 108, 111, 32, 84, 111, 107, 101, 110, 105, 122, 101, 114, 39, 115, 32, 84, 111, 107, 101, 110, 105, 122, 101, 114, 226, 128, 153, 115, 32, 84, 79, 75, 69, 78, 73, 90, 69, 82, 39, 83, 32, 84, 111, 107, 101, 110, 105, 122, 101, 114, 39, 83]\n",
      "Decoded: Hello Tokenizer's Tokenizer‚Äôs TOKENIZER'S Tokenizer'S\n",
      "--------------------\n",
      "Size: 36\n",
      "‚úîÔ∏è Encoded: [72, 463, 321, 330, 1513, 286, 1470, 290, 39, 115, 330, 1513, 286, 1470, 290, 3988, 153, 115, 330, 79, 75, 69, 78, 73, 90, 69, 82, 39, 83, 330, 1513, 286, 1470, 290, 39, 83]\n",
      "Decoded: Hello Tokenizer's Tokenizer‚Äôs TOKENIZER'S Tokenizer'S\n",
      "--------------------\n",
      "Size: 36\n",
      "‚òëÔ∏è Encoded: [72, 472, 321, 332, 1743, 285, 1609, 289, 39, 115, 332, 1743, 285, 1609, 289, 4584, 153, 115, 332, 79, 75, 69, 78, 73, 90, 69, 82, 39, 83, 332, 1743, 285, 1609, 289, 39, 83]\n",
      "Decoded: Hello Tokenizer's Tokenizer‚Äôs TOKENIZER'S Tokenizer'S\n"
     ]
    }
   ],
   "source": [
    "str = \"Hello Tokenizer's Tokenizer‚Äôs TOKENIZER'S Tokenizer'S\"\n",
    "\n",
    "# ‚úñÔ∏è Vocabularies\n",
    "encoded = tokenizer_without_files.encode(str)\n",
    "decoded = tokenizer_without_files.decode(encoded)\n",
    "print(f\"Size: {len(encoded)}\")\n",
    "print(\"‚úñÔ∏è Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)\n",
    "\n",
    "print('-' * 20)\n",
    "\n",
    "# ‚úîÔ∏è Vocabularies (GPT2 Pattern)\n",
    "encoded = my_basic_tokenizer.encode(str)\n",
    "decoded = my_basic_tokenizer.decode(encoded)\n",
    "print(f\"Size: {len(encoded)}\")\n",
    "print(\"‚úîÔ∏è Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)\n",
    "\n",
    "print('-' * 20)\n",
    "\n",
    "# ‚òëÔ∏è Vocabularies Ameliored (GPT4 Pattern)\n",
    "encoded = my_tokenizer.encode(str)\n",
    "decoded = my_tokenizer.decode(encoded)\n",
    "print(f\"Size: {len(encoded)}\")\n",
    "print(\"‚òëÔ∏è Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d3cdfdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 1527\n",
      "‚úîÔ∏è Encoded: [10, 102, 424, 426, 525, 46, 497, 572, 46, 117, 777, 46, 613, 300, 298, 571, 582, 115, 95, 1848, 99, 10, 102, 424, 426, 525, 46, 497, 572, 46, 117, 777, 46, 263, 1648, 298, 571, 509, 1194, 95, 846, 115, 10, 102, 424, 426, 525, 46, 497, 572, 46, 117, 777, 46, 116, 102, 95, 453, 672, 298, 571, 2757, 846, 95, 258, 415, 10, 102, 424, 426, 525, 46, 497, 572, 46, 117, 777, 46, 116, 102, 95, 453, 672, 298, 571, 259, 102, 95, 453, 672, 10, 10, 95, 84, 32, 61, 1978, 375, 526, 95, 84, 34, 41, 10, 469, 676, 32, 61, 1978, 375, 40, 34, 469, 676, 34, 44, 317, 1859, 61, 34, 71, 302, 34, 41, 10, 2420, 676, 32, 61, 1978, 375, 40, 34, 2420, 676, 34, 44, 317, 1859, 61, 34, 524, 1492, 34, 41, 10, 459, 676, 32, 61, 1978, 375, 40, 34, 459, 676, 34, 44, 317, 1859, 61, 34, 79, 385, 34, 41, 10, 759, 676, 32, 61, 1978, 375, 40, 34, 759, 676, 34, 44, 317, 1859, 1081, 95, 759, 1988, 34, 41, 10, 10, 10, 35, 1082, 79, 40, 98, 47, 4084, 53, 41, 58, 1079, 1158, 270, 1103, 833, 103, 32, 275, 4086, 380, 46, 10, 105, 115, 95, 111, 356, 32, 61, 330, 638, 32, 32, 35, 4089, 866, 317, 121, 4091, 2759, 10, 10, 35, 4094, 3212, 314, 928, 1558, 386, 4097, 315, 298, 102, 311, 101, 867, 761, 303, 576, 274, 101, 311, 857, 45, 105, 110, 45, 4099, 356, 10, 35, 682, 115, 259, 111, 274, 101, 32, 67, 377, 1058, 46, 466, 265, 369, 430, 317, 101, 1752, 1033, 543, 287, 270, 430, 447, 2427, 32, 275, 859, 1305, 46, 10, 95, 85, 1972, 95, 67, 95, 65, 1058, 58, 317, 880, 32, 61, 330, 638, 10, 95, 85, 1972, 95, 67, 95, 4103, 2761, 58, 317, 880, 32, 61, 330, 638, 10, 10, 10, 95, 738, 105, 95, 117, 1989, 95, 1753, 1754, 32, 61, 2414, 2180, 46, 2764, 1754, 40, 10, 260, 363, 47, 451, 525, 47, 738, 105, 47, 357, 115, 95, 101, 446, 95, 929, 278, 34, 44, 10, 260, 32, 34, 87, 2429, 370, 115, 46, 101, 576, 95, 101, 446, 95, 929, 278, 40, 41, 32, 275, 529, 868, 46, 478, 10, 10, 95, 364, 402, 95, 102, 432, 95, 738, 105, 95, 1753, 1754, 32, 61, 2414, 2180, 46, 2764, 1754, 40, 10, 260, 363, 47, 451, 525, 47, 738, 105, 47, 101, 576, 95, 364, 402, 95, 102, 432, 95, 118, 50, 34, 44, 10, 260, 32, 34, 87, 2429, 303, 576, 95, 364, 402, 95, 102, 432, 95, 118, 50, 40, 41, 32, 275, 529, 868, 46, 478, 10, 10, 95, 116, 102, 95, 102, 352, 95, 738, 105, 95, 1753, 1754, 32, 61, 2414, 2180, 46, 2764, 1754, 40, 10, 260, 363, 47, 451, 525, 47, 738, 105, 47, 116, 102, 95, 102, 352, 34, 44, 10, 260, 32, 34, 87, 2429, 259, 102, 46, 102, 352, 40, 41, 32, 275, 442, 100, 46, 478, 10, 10, 35, 475, 444, 58, 494, 366, 61, 547, 482, 45, 322, 498, 10, 95, 3216, 2761, 95, 2431, 78, 95, 84, 780, 58, 644, 527, 91, 340, 115, 95, 112, 98, 50, 46, 3218, 676, 44, 568, 115, 46, 68, 676, 93, 32, 61, 32, 40, 10, 260, 568, 115, 46, 95, 2431, 78, 95, 84, 780, 41, 10, 35, 475, 444, 58, 303, 576, 61, 547, 482, 45, 322, 498, 10, 10, 10, 263, 102, 259, 331, 95, 105, 100, 40, 439, 301, 41, 388, 62, 989, 121, 58, 10, 32, 32, 291, 470, 115, 32, 97, 659, 1105, 4105, 290, 292, 261, 274, 275, 330, 331, 46, 291, 10, 32, 307, 329, 259, 331, 46, 95, 105, 100, 32, 32, 35, 475, 444, 58, 494, 366, 61, 547, 482, 45, 322, 498, 10, 10, 10, 99, 731, 32, 95, 2767, 1756, 40, 111, 992, 41, 58, 10, 32, 32, 291, 778, 546, 442, 114, 45, 2770, 1560, 32, 371, 342, 100, 1233, 263, 561, 1307, 281, 102, 3220, 380, 32, 371, 46, 291, 10, 277, 32, 295, 397, 95, 262, 558, 534, 40, 293, 102, 44, 32, 371, 95, 279, 276, 95, 111, 114, 95, 102, 352, 41, 388, 62, 32, 344, 58, 10, 260, 32, 294, 46, 95, 263, 367, 95, 279, 276, 95, 111, 114, 95, 102, 352, 32, 61, 32, 371, 95, 279, 276, 95, 111, 114, 95, 102, 352, 10, 260, 32, 294, 46, 1013, 2187, 95, 279, 276, 32, 61, 32, 400, 40, 293, 102, 46, 95, 263, 367, 95, 279, 276, 95, 111, 114, 95, 102, 352, 41, 10, 260, 32, 294, 46, 102, 352, 32, 61, 32, 371, 95, 279, 276, 95, 111, 114, 95, 102, 352, 10, 260, 32, 294, 46, 271, 119, 95, 400, 315, 32, 61, 32, 344, 10, 264, 298, 102, 318, 662, 40, 263, 367, 95, 279, 276, 95, 111, 114, 95, 102, 352, 44, 428, 1488, 46, 3222, 1559, 41, 58, 269, 32, 32, 294, 46, 105, 115, 95, 110, 1100, 95, 1273, 101, 32, 61, 32, 371, 95, 279, 276, 95, 111, 114, 95, 102, 352, 46, 105, 115, 95, 110, 1100, 95, 1273, 101, 10, 264, 303, 834, 682, 366, 40, 263, 367, 95, 279, 276, 95, 111, 114, 95, 102, 352, 41, 58, 269, 32, 32, 294, 46, 105, 115, 95, 110, 1100, 95, 1273, 101, 32, 61, 550, 711, 269, 32, 509, 118, 95, 102, 655, 32, 61, 32, 294, 46, 95, 263, 367, 95, 279, 276, 95, 111, 114, 95, 102, 352, 269, 32, 292, 655, 95, 279, 276, 32, 61, 292, 352, 95, 811, 115, 46, 103, 416, 95, 102, 655, 95, 279, 276, 40, 263, 118, 95, 102, 655, 41, 269, 32, 292, 655, 95, 320, 263, 32, 61, 292, 352, 95, 811, 115, 46, 103, 416, 95, 102, 655, 95, 320, 263, 40, 263, 118, 95, 102, 655, 41, 269, 32, 298, 102, 292, 655, 95, 320, 263, 58, 269, 260, 292, 289, 32, 61, 292, 655, 95, 320, 263, 46, 99, 111, 95, 1992, 289, 269, 260, 443, 1759, 32, 61, 292, 655, 95, 320, 263, 46, 99, 111, 95, 4110, 2775, 269, 32, 597, 265, 58, 269, 260, 292, 289, 32, 61, 32, 34, 304, 2776, 34, 269, 260, 443, 1759, 32, 61, 32, 45, 49, 269, 32, 32, 294, 46, 1013, 2187, 95, 279, 276, 32, 61, 363, 37, 115, 60, 37, 115, 44, 32, 37, 100, 62, 34, 32, 37, 32, 40, 102, 655, 95, 279, 276, 44, 292, 289, 44, 443, 1759, 41, 10, 264, 303, 834, 32, 371, 95, 279, 276, 95, 111, 114, 95, 102, 352, 32, 275, 32, 344, 58, 269, 32, 32, 35, 549, 1352, 40, 4118, 101, 41, 58, 407, 275, 4120, 3224, 317, 101, 550, 711, 46, 32, 344, 4122, 1353, 32, 97, 1865, 107, 32, 262, 274, 101, 269, 32, 32, 35, 32, 32, 32, 371, 32, 387, 44, 323, 111, 32, 96, 105, 115, 95, 110, 1100, 95, 1273, 101, 96, 346, 620, 317, 101, 550, 711, 292, 261, 881, 465, 32, 97, 273, 760, 259, 111, 269, 32, 32, 35, 32, 32, 623, 432, 1308, 115, 259, 111, 2777, 415, 1761, 1160, 281, 427, 312, 1100, 3220, 386, 413, 590, 2436, 828, 32, 97, 32, 344, 46, 269, 32, 32, 294, 46, 105, 115, 95, 110, 1100, 95, 1273, 101, 32, 61, 550, 711, 10, 264, 597, 265, 58, 269, 32, 32, 294, 46, 271, 119, 95, 400, 315, 32, 61, 32, 371, 95, 279, 276, 95, 111, 114, 95, 102, 352, 269, 32, 32, 294, 46, 102, 352, 32, 61, 428, 1488, 46, 1273, 101, 95, 263, 367, 40, 263, 367, 95, 279, 276, 95, 111, 114, 95, 102, 352, 41, 269, 32, 32, 294, 46, 105, 115, 95, 110, 1100, 95, 1273, 101, 32, 61, 32, 294, 46, 102, 352, 46, 105, 115, 95, 110, 1100, 95, 1273, 101, 10, 264, 32, 35, 944, 101, 1098, 1156, 274, 275, 949, 1014, 32, 262, 397, 95, 262, 558, 95, 95, 317, 1651, 298, 116, 32, 275, 281, 102, 312, 258, 45, 4128, 282, 378, 283, 44, 10, 260, 32, 35, 342, 100, 32, 294, 46, 400, 315, 95, 1273, 101, 32, 275, 2438, 931, 529, 868, 1109, 121, 1565, 115, 46, 10, 260, 32, 294, 46, 102, 1303, 95, 400, 315, 95, 1273, 101, 32, 61, 318, 662, 40, 293, 102, 46, 102, 352, 44, 428, 1488, 46, 3222, 1559, 41, 10, 277, 32, 295, 489, 315, 95, 1273, 101, 40, 293, 102, 44, 341, 263, 95, 263, 102, 41, 388, 62, 32, 400, 58, 10, 260, 298, 102, 32, 294, 46, 102, 1303, 95, 400, 315, 95, 1273, 101, 58, 269, 32, 307, 329, 32, 294, 46, 102, 352, 46, 2778, 3228, 95, 400, 315, 95, 1273, 101, 40, 110, 445, 95, 263, 102, 41, 10, 264, 307, 329, 561, 300, 46, 97, 115, 95, 283, 114, 40, 95, 263, 367, 95, 400, 315, 40, 293, 102, 46, 102, 352, 40, 110, 445, 95, 263, 102, 610, 41, 10, 10]\n",
      "--------------------\n",
      "Size: 1350\n",
      "‚òëÔ∏è Encoded: [10, 102, 426, 429, 613, 46, 650, 46, 1469, 46, 905, 297, 586, 603, 115, 3124, 99, 10, 102, 426, 429, 613, 46, 650, 46, 1469, 1930, 1805, 297, 586, 518, 1292, 1169, 115, 10, 102, 426, 429, 613, 46, 650, 46, 1469, 46, 411, 601, 702, 297, 586, 1627, 2061, 1293, 417, 10, 102, 426, 429, 613, 46, 650, 46, 1469, 46, 411, 601, 702, 297, 586, 259, 102, 601, 702, 10, 10, 95, 84, 32, 61, 2201, 374, 535, 95, 84, 34, 331, 478, 703, 32, 61, 2201, 374, 40, 34, 478, 703, 34, 44, 317, 2063, 61, 34, 71, 301, 34, 331, 2732, 703, 32, 61, 2201, 374, 40, 34, 2732, 703, 34, 44, 317, 2063, 61, 34, 534, 1629, 34, 331, 466, 703, 32, 61, 2201, 374, 40, 34, 466, 703, 34, 44, 317, 2063, 61, 34, 79, 385, 34, 331, 798, 703, 32, 61, 2201, 374, 40, 34, 798, 703, 34, 44, 317, 2063, 1170, 95, 798, 2213, 34, 856, 35, 1171, 79, 40, 98, 47, 2733, 55, 3651, 52, 4687, 53, 41, 58, 1165, 1250, 268, 1191, 879, 103, 32, 273, 4689, 379, 46, 10, 105, 115, 520, 356, 32, 61, 332, 661, 32, 32, 35, 4692, 917, 317, 121, 4694, 3127, 10, 10, 35, 4697, 3654, 314, 982, 1703, 386, 4700, 315, 297, 102, 310, 101, 919, 800, 302, 594, 272, 101, 310, 904, 45, 262, 4704, 356, 10, 35, 708, 115, 259, 111, 272, 101, 32, 67, 376, 1138, 46, 475, 264, 369, 435, 317, 101, 1932, 1109, 553, 286, 268, 435, 454, 2738, 32, 273, 907, 1420, 46, 10, 3129, 2195, 95, 67, 2214, 1138, 58, 317, 929, 32, 61, 332, 661, 10, 3129, 2195, 95, 67, 4708, 3130, 58, 317, 929, 32, 61, 332, 661, 282, 10, 95, 1381, 733, 2215, 857, 1933, 32, 61, 2727, 2439, 3133, 1933, 40, 10, 256, 32, 363, 47, 457, 613, 47, 1381, 47, 3135, 459, 453, 601, 1040, 34, 427, 256, 32, 32, 34, 87, 2739, 370, 115, 1467, 594, 459, 453, 601, 1040, 40, 41, 32, 273, 539, 920, 880, 451, 483, 403, 400, 437, 95, 1381, 857, 1933, 32, 61, 2727, 2439, 3133, 1933, 40, 10, 256, 32, 363, 47, 457, 613, 47, 1381, 47, 3657, 483, 403, 400, 437, 95, 118, 50, 34, 427, 256, 32, 32, 34, 87, 2739, 302, 594, 483, 403, 400, 437, 95, 118, 50, 40, 41, 32, 273, 539, 920, 880, 451, 95, 411, 400, 352, 95, 1381, 857, 1933, 32, 61, 2727, 2439, 3133, 1933, 40, 10, 256, 32, 363, 47, 457, 613, 47, 1381, 47, 411, 400, 352, 34, 427, 256, 32, 32, 34, 87, 2739, 259, 102, 649, 352, 40, 41, 32, 273, 448, 100, 880, 451, 35, 485, 450, 58, 506, 366, 558, 493, 45, 554, 10, 2449, 3660, 2066, 2741, 2212, 821, 58, 666, 537, 2450, 115, 593, 98, 50, 2217, 3662, 44, 584, 115, 46, 1194, 93, 32, 61, 442, 10, 256, 32, 584, 115, 46, 95, 1135, 2741, 2212, 821, 41, 10, 35, 485, 450, 58, 302, 594, 558, 493, 45, 554, 282, 10, 263, 102, 259, 333, 95, 1002, 40, 457, 41, 387, 62, 1053, 121, 58, 10, 32, 32, 290, 479, 115, 32, 97, 683, 1195, 4715, 289, 291, 261, 272, 273, 332, 333, 46, 290, 10, 32, 306, 330, 259, 333, 46, 95, 105, 100, 32, 32, 35, 485, 450, 58, 506, 366, 558, 493, 45, 554, 282, 10, 99, 768, 32, 95, 3137, 1935, 2069, 1056, 41, 276, 32, 32, 290, 819, 556, 448, 114, 3664, 1706, 32, 371, 342, 100, 1338, 263, 575, 1422, 280, 102, 3665, 379, 32, 371, 46, 290, 282, 32, 32, 294, 398, 95, 262, 570, 543, 40, 292, 102, 44, 32, 371, 95, 288, 95, 261, 400, 352, 41, 387, 62, 32, 344, 58, 10, 256, 32, 32, 293, 46, 95, 263, 367, 95, 288, 95, 261, 400, 352, 32, 61, 32, 371, 95, 288, 95, 261, 400, 352, 10, 256, 32, 32, 293, 3141, 2453, 95, 288, 32, 61, 396, 114, 40, 293, 46, 95, 263, 367, 95, 288, 95, 261, 400, 352, 41, 10, 256, 32, 32, 293, 649, 352, 32, 61, 32, 371, 95, 288, 95, 261, 400, 352, 10, 256, 32, 32, 293, 46, 2220, 844, 315, 32, 61, 32, 344, 10, 10, 256, 32, 297, 102, 318, 685, 40, 371, 95, 288, 95, 261, 400, 352, 44, 433, 1624, 3667, 1705, 41, 276, 257, 32, 32, 293, 46, 273, 709, 1189, 1218, 1385, 32, 61, 32, 371, 95, 288, 95, 261, 400, 352, 46, 273, 709, 1189, 1218, 1385, 10, 10, 256, 32, 302, 882, 708, 366, 40, 371, 95, 288, 95, 261, 400, 352, 41, 276, 257, 32, 32, 293, 46, 273, 709, 1189, 1218, 1385, 32, 61, 562, 743, 10, 257, 32, 518, 118, 400, 677, 32, 61, 32, 293, 46, 95, 263, 367, 95, 288, 95, 261, 400, 352, 10, 257, 32, 291, 677, 95, 288, 32, 61, 291, 352, 1036, 115, 46, 572, 400, 677, 95, 288, 40, 1624, 400, 677, 41, 10, 257, 32, 291, 677, 597, 263, 32, 61, 291, 352, 1036, 115, 46, 572, 400, 677, 597, 263, 40, 1624, 400, 677, 41, 10, 257, 32, 297, 102, 291, 677, 597, 263, 58, 10, 257, 260, 291, 288, 32, 61, 291, 677, 597, 263, 46, 320, 3670, 288, 10, 257, 260, 449, 1939, 32, 61, 291, 677, 597, 263, 46, 320, 4720, 3145, 10, 257, 32, 618, 264, 58, 10, 257, 260, 291, 288, 32, 61, 32, 34, 303, 3146, 34, 10, 257, 260, 449, 1939, 32, 61, 32, 45, 49, 10, 257, 32, 32, 293, 3141, 2453, 95, 288, 32, 61, 363, 37, 115, 60, 37, 115, 44, 32, 37, 100, 62, 34, 32, 37, 32, 40, 102, 677, 95, 288, 44, 291, 288, 44, 449, 1939, 41, 282, 256, 32, 302, 882, 32, 371, 95, 288, 95, 261, 400, 352, 32, 273, 32, 344, 58, 10, 257, 32, 32, 35, 561, 1473, 4729, 101, 41, 58, 409, 273, 4731, 3671, 317, 101, 562, 743, 46, 32, 344, 4733, 1474, 32, 97, 2074, 107, 32, 262, 272, 101, 10, 257, 32, 32, 35, 32, 32, 32, 371, 396, 354, 44, 323, 111, 32, 96, 105, 115, 709, 1189, 1218, 1385, 96, 346, 640, 317, 101, 562, 743, 291, 261, 930, 474, 32, 97, 271, 799, 259, 111, 10, 257, 32, 32, 35, 32, 32, 644, 437, 1423, 115, 259, 111, 3147, 417, 1941, 1252, 280, 432, 311, 1189, 3665, 386, 414, 965, 2749, 873, 32, 97, 32, 344, 46, 10, 257, 32, 32, 293, 46, 273, 709, 1189, 1218, 1385, 32, 61, 562, 743, 10, 10, 256, 32, 618, 264, 58, 10, 257, 32, 32, 293, 46, 2220, 844, 315, 32, 61, 32, 371, 95, 288, 95, 261, 400, 352, 10, 257, 32, 32, 293, 649, 352, 32, 61, 433, 1624, 1806, 1385, 95, 371, 40, 371, 95, 288, 95, 261, 400, 352, 41, 10, 257, 32, 32, 293, 46, 273, 709, 1189, 1218, 1385, 32, 61, 32, 293, 649, 352, 46, 273, 709, 1189, 1218, 1385, 10, 10, 256, 32, 32, 35, 1001, 101, 1188, 1168, 272, 273, 1006, 1087, 32, 262, 398, 95, 262, 570, 95, 95, 317, 1807, 297, 116, 32, 273, 280, 102, 311, 258, 4740, 281, 377, 283, 44, 10, 256, 32, 32, 35, 342, 100, 32, 293, 46, 1523, 1218, 1385, 32, 273, 2751, 984, 539, 920, 1199, 121, 1712, 115, 46, 10, 256, 32, 32, 293, 649, 1418, 844, 315, 1218, 1385, 32, 61, 318, 685, 40, 293, 649, 352, 44, 433, 1624, 3667, 1705, 41, 282, 32, 32, 294, 501, 315, 1218, 1385, 40, 293, 44, 341, 263, 95, 294, 41, 387, 62, 396, 114, 58, 10, 256, 32, 297, 102, 32, 293, 649, 1418, 844, 315, 1218, 1385, 58, 10, 257, 32, 306, 330, 32, 293, 649, 352, 3676, 3678, 844, 315, 1218, 1385, 40, 966, 95, 294, 41, 282, 256, 32, 306, 330, 575, 299, 46, 464, 384, 114, 40, 95, 263, 367, 844, 315, 40, 293, 649, 352, 40, 966, 95, 294, 41, 1388]\n"
     ]
    }
   ],
   "source": [
    "python_code = \"\"\"\n",
    "from tensorflow.python.util.compat import collections_abc\n",
    "from tensorflow.python.util.deprecation import deprecated_args\n",
    "from tensorflow.python.util.tf_export import kwarg_only\n",
    "from tensorflow.python.util.tf_export import tf_export\n",
    "\n",
    "_T = TypeVar(\"_T\")\n",
    "GraphType = TypeVar(\"GraphType\", bound=\"Graph\")\n",
    "OpStatsType = TypeVar(\"OpStatsType\", bound=\"OpStats\")\n",
    "OperationType = TypeVar(\"OperationType\", bound=\"Operation\")\n",
    "EagerTensorType = TypeVar(\"EagerTensorType\", bound=\"_EagerTensorBase\")\n",
    "\n",
    "\n",
    "# TODO(b/307794935): Remove after bug is fixed.\n",
    "is_oss = True  # Updated by copybara\n",
    "\n",
    "# Temporary global switches determining if we should enable the work-in-progress\n",
    "# calls to the C API. These will be removed once all functionality is supported.\n",
    "_USE_C_API: bool = True\n",
    "_USE_C_SHAPES: bool = True\n",
    "\n",
    "\n",
    "_api_usage_gauge = monitoring.BoolGauge(\n",
    "    \"/tensorflow/api/ops_eager_execution\",\n",
    "    \"Whether ops.enable_eager_execution() is called.\")\n",
    "\n",
    "_control_flow_api_gauge = monitoring.BoolGauge(\n",
    "    \"/tensorflow/api/enable_control_flow_v2\",\n",
    "    \"Whether enable_control_flow_v2() is called.\")\n",
    "\n",
    "_tf_function_api_gauge = monitoring.BoolGauge(\n",
    "    \"/tensorflow/api/tf_function\",\n",
    "    \"Whether tf.function() is used.\")\n",
    "\n",
    "# pylint: disable=protected-access\n",
    "_DTYPES_INTERN_TABLE: dict[types_pb2.DataType, dtypes.DType] = (\n",
    "    dtypes._INTERN_TABLE)\n",
    "# pylint: enable=protected-access\n",
    "\n",
    "\n",
    "def tensor_id(tensor) -> Any:\n",
    "  \"\"Returns a unique identifier for this Tensor.\"\"\n",
    "  return tensor._id  # pylint: disable=protected-access\n",
    "\n",
    "\n",
    "class _UserDeviceSpec(object):\n",
    "  \"\"Store user-specified device and provide computation of merged device.\"\"\n",
    "\n",
    "  def __init__(self, device_name_or_function) -> None:\n",
    "    self._device_name_or_function = device_name_or_function\n",
    "    self.display_name = str(self._device_name_or_function)\n",
    "    self.function = device_name_or_function\n",
    "    self.raw_string = None\n",
    "\n",
    "    if isinstance(device_name_or_function, pydev.MergeDevice):\n",
    "      self.is_null_merge = device_name_or_function.is_null_merge\n",
    "\n",
    "    elif callable(device_name_or_function):\n",
    "      self.is_null_merge = False\n",
    "      dev_func = self._device_name_or_function\n",
    "      func_name = function_utils.get_func_name(dev_func)\n",
    "      func_code = function_utils.get_func_code(dev_func)\n",
    "      if func_code:\n",
    "        fname = func_code.co_filename\n",
    "        lineno = func_code.co_firstlineno\n",
    "      else:\n",
    "        fname = \"unknown\"\n",
    "        lineno = -1\n",
    "      self.display_name = \"%s<%s, %d>\" % (func_name, fname, lineno)\n",
    "\n",
    "    elif device_name_or_function is None:\n",
    "      # NOTE(taylorrobie): This MUST be False. None signals a break in the\n",
    "      #   device stack, so `is_null_merge` must be False for such a case to\n",
    "      #   allow callers to safely skip over null merges without missing a None.\n",
    "      self.is_null_merge = False\n",
    "\n",
    "    else:\n",
    "      self.raw_string = device_name_or_function\n",
    "      self.function = pydev.merge_device(device_name_or_function)\n",
    "      self.is_null_merge = self.function.is_null_merge\n",
    "\n",
    "    # We perform this check in __init__ because it is of non-trivial cost,\n",
    "    # and self.string_merge is typically called many times.\n",
    "    self.fast_string_merge = isinstance(self.function, pydev.MergeDevice)\n",
    "\n",
    "  def string_merge(self, node_def) -> str:\n",
    "    if self.fast_string_merge:\n",
    "      return self.function.shortcut_string_merge(node_def)\n",
    "\n",
    "    return compat.as_str(_device_string(self.function(node_def)))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ‚úîÔ∏è Vocabularies\n",
    "encoded = my_basic_tokenizer.encode(python_code)\n",
    "decoded = my_basic_tokenizer.decode(encoded)\n",
    "print(f\"Size: {len(encoded)}\")\n",
    "print(\"‚úîÔ∏è Encoded:\", encoded)\n",
    "\n",
    "print('-' * 20)\n",
    "\n",
    "# ‚òëÔ∏è Vocabularies Ameliored\n",
    "encoded = my_tokenizer.encode(python_code)\n",
    "decoded = my_tokenizer.decode(encoded)\n",
    "print(f\"Size: {len(encoded)}\")\n",
    "print(\"‚òëÔ∏è Encoded:\", encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb34c6",
   "metadata": {},
   "source": [
    "### Test the new pattern used for GPT4o\n",
    "\n",
    "The name's called o200k_base, it used 200k of tokens for vocabularies. Here we used a little part of data for the trainning with only 5k tokens\n",
    "\n",
    "#### My Tokenizer v3 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "67b3df05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading tokenizer...\n",
      "‚ö†Ô∏è Model file 'my_advanced_tokenizer.model' not found. It will be created after training.\n",
      "üõ†Ô∏è Training tokenizer...\n",
      "‚öôÔ∏è Merge 744: (46, 357) -> 1000\n",
      "‚öôÔ∏è Merge 1744: (67, 1367) -> 2000\n",
      "‚öôÔ∏è Merge 2744: (2154, 34) -> 3000\n",
      "‚öôÔ∏è Merge 3744: (1762, 417) -> 4000\n",
      "üíæ my_tokenizer_v3 saved to my_advanced_tokenizer.model and my_advanced_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "my_advanced_tokenizer = MyTokenizer(vocab_size=5000, pattern=GPT4O_SPLIT_PATTERN)\n",
    "# Load the tokenizer from files\n",
    "my_advanced_tokenizer.load('my_advanced_tokenizer')\n",
    "\n",
    "# Train the tokenizer on a text file\n",
    "with open('data_code.txt', 'r', encoding='utf-8') as f:\n",
    "    my_advanced_tokenizer.train(f.read())\n",
    "\n",
    "# Save the tokenizer vocab and encoder files\n",
    "my_advanced_tokenizer.save('my_advanced_tokenizer', version_name='my_tokenizer_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "65496256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 329\n",
      "‚úîÔ∏è Encoded: [239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 3988, 189, 32, 240, 159, 135, 186, 3988, 140, 240, 159, 135, 179, 3988, 140, 240, 159, 135, 174, 3988, 140, 240, 159, 135, 168, 3988, 140, 240, 159, 135, 180, 3988, 140, 240, 159, 135, 169, 3988, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 407, 101, 32, 2044, 32, 289, 489, 105, 1464, 292, 101, 375, 342, 100, 270, 1469, 299, 408, 274, 101, 596, 101, 375, 468, 281, 102, 1765, 276, 114, 115, 4318, 108, 100, 119, 2095, 46, 944, 101, 270, 430, 1491, 1561, 311, 101, 972, 1671, 116, 259, 111, 32, 3988, 156, 115, 510, 672, 1262, 105, 1275, 3988, 157, 32, 262, 281, 306, 4023, 1550, 32, 40, 119, 393, 267, 427, 274, 300, 2340, 115, 3988, 148, 390, 733, 412, 828, 311, 465, 375, 95, 116, 292, 261, 270, 430, 274, 101, 600, 115, 44, 1411, 1844, 63, 715, 1223, 309, 1262, 105, 1275, 273, 349, 317, 101, 1129, 809, 265, 44, 342, 100, 644, 2128, 299, 408, 274, 101, 274, 1472, 661, 45, 1786, 687, 1262, 105, 1275, 3532, 661, 920, 4545, 996, 298, 468, 629, 122, 286, 115, 281, 102, 859, 112, 720, 853, 342, 110, 453, 386, 44, 4514, 115, 44, 342, 100, 341, 836, 273, 349, 317, 101, 346, 546, 274, 349, 32, 97, 2407, 1483, 280, 299, 1714, 945, 1613, 46, 32, 73, 345, 258, 3988, 153, 116, 317, 519, 276, 1765, 276, 114, 115, 292, 261, 2107, 430, 1819, 694, 274, 101, 472, 111, 280, 274, 315, 1837, 115, 425, 105, 1472, 44, 303, 1586, 2056, 48, 679, 101, 375, 115, 270, 1103, 1262, 105, 1275, 3988, 153, 115, 299, 1443, 46]\n",
      "--------------------\n",
      "Size: 332\n",
      "‚òëÔ∏è Encoded: [239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 4584, 189, 32, 240, 159, 135, 186, 4584, 140, 240, 159, 135, 179, 4584, 140, 240, 159, 135, 174, 4584, 140, 240, 159, 135, 168, 4584, 140, 240, 159, 135, 180, 4584, 140, 240, 159, 135, 169, 4584, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 409, 101, 32, 2295, 32, 288, 501, 105, 1600, 291, 101, 374, 342, 100, 268, 1608, 298, 410, 272, 101, 4859, 374, 477, 280, 102, 1946, 274, 114, 115, 310, 261, 108, 100, 119, 2343, 46, 1001, 101, 268, 435, 1627, 1707, 310, 101, 280, 117, 1834, 116, 259, 111, 32, 4584, 156, 115, 519, 702, 1373, 105, 2643, 4584, 157, 32, 262, 280, 305, 4624, 1696, 32, 40, 119, 393, 266, 432, 272, 299, 2642, 115, 4584, 148, 1258, 413, 873, 310, 474, 374, 95, 116, 291, 261, 268, 435, 272, 101, 621, 115, 44, 1542, 2048, 63, 1387, 1326, 308, 1373, 105, 2643, 271, 349, 317, 101, 3119, 851, 264, 44, 342, 100, 666, 2380, 298, 410, 272, 101, 272, 1612, 630, 45, 112, 1080, 1373, 105, 2643, 4055, 1556, 325, 108, 1064, 297, 477, 653, 122, 285, 115, 280, 102, 907, 112, 755, 897, 342, 110, 598, 386, 44, 306, 702, 115, 44, 342, 100, 341, 892, 271, 349, 317, 101, 346, 556, 272, 349, 32, 97, 2717, 1620, 279, 298, 1888, 1002, 1761, 46, 32, 73, 345, 258, 4584, 153, 116, 317, 527, 274, 1946, 274, 114, 115, 291, 261, 2355, 435, 291, 262, 724, 272, 101, 481, 111, 279, 272, 315, 2038, 115, 428, 105, 1612, 44, 1395, 285, 32, 51, 48, 707, 101, 374, 115, 268, 1191, 1373, 105, 2643, 4584, 153, 115, 298, 1578, 46]\n",
      "--------------------\n",
      "Size: 331\n",
      "‚òëÔ∏è‚òëÔ∏è Encoded: [239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 4484, 189, 32, 240, 159, 135, 186, 4484, 140, 240, 159, 135, 179, 4484, 140, 240, 159, 135, 174, 4484, 140, 240, 159, 135, 168, 4484, 140, 240, 159, 135, 180, 4484, 140, 240, 159, 135, 169, 4484, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 409, 101, 32, 2265, 32, 288, 501, 105, 1586, 291, 101, 374, 342, 100, 268, 1594, 298, 410, 272, 101, 4756, 374, 477, 280, 102, 1926, 274, 114, 115, 4896, 108, 100, 119, 2313, 46, 998, 101, 268, 435, 1613, 1690, 310, 101, 280, 117, 1816, 116, 259, 111, 32, 4484, 156, 115, 519, 701, 1364, 105, 2605, 4484, 157, 32, 262, 280, 305, 4524, 1680, 32, 40, 119, 393, 266, 432, 272, 299, 2604, 115, 4484, 148, 1249, 413, 870, 310, 474, 374, 95, 116, 291, 261, 268, 435, 272, 101, 621, 115, 44, 1530, 2025, 63, 1378, 1316, 308, 1364, 105, 2605, 271, 349, 317, 101, 3064, 848, 264, 44, 342, 100, 665, 2350, 298, 410, 272, 101, 272, 1598, 630, 45, 112, 1075, 1364, 105, 2605, 3967, 1544, 325, 108, 1060, 297, 477, 652, 122, 285, 115, 280, 102, 905, 112, 753, 895, 342, 110, 598, 386, 44, 306, 701, 115, 44, 342, 100, 341, 890, 271, 349, 317, 101, 346, 556, 272, 349, 32, 97, 2677, 1606, 279, 298, 1869, 999, 1744, 46, 32, 73, 345, 258, 4484, 153, 116, 317, 527, 274, 1926, 274, 114, 115, 291, 261, 2325, 435, 291, 262, 722, 272, 101, 481, 111, 279, 272, 315, 2016, 115, 428, 105, 1598, 44, 1386, 285, 32, 51, 48, 706, 101, 374, 115, 268, 1184, 1364, 105, 2605, 4484, 153, 115, 298, 1566, 46]\n"
     ]
    }
   ],
   "source": [
    "text = \"ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\"\n",
    "\n",
    "\n",
    "# ‚úîÔ∏è Vocabularies\n",
    "encoded = my_basic_tokenizer.encode(text)\n",
    "decoded = my_basic_tokenizer.decode(encoded)\n",
    "print(f\"Size: {len(encoded)}\")\n",
    "print(\"‚úîÔ∏è Encoded:\", encoded)\n",
    "\n",
    "print('-' * 20)\n",
    "\n",
    "# ‚òëÔ∏è Vocabularies Ameliored\n",
    "encoded = my_tokenizer.encode(text)\n",
    "decoded = my_tokenizer.decode(encoded)\n",
    "print(f\"Size: {len(encoded)}\")\n",
    "print(\"‚òëÔ∏è Encoded:\", encoded)\n",
    "\n",
    "print('-' * 20)\n",
    "\n",
    "# ‚òëÔ∏è‚òëÔ∏è Vocabularies Advanced\n",
    "encoded = my_advanced_tokenizer.encode(text)\n",
    "decoded = my_advanced_tokenizer.decode(encoded)\n",
    "print(f\"Size: {len(encoded)}\")\n",
    "print(\"‚òëÔ∏è‚òëÔ∏è Encoded:\", encoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
